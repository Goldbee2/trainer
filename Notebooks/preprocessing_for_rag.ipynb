{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354d43a7-21d4-43c4-b6a3-8a05c06a66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d1bec-a5c8-41ef-8038-829f83462aab",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "This will read, filter, clean, and combine my data for chunking and ingestion for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db5ea67-5454-4051-a780-adfdd7656550",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_zst_path = \"../Data/climbharder_submissions.zst\" #local path to file\n",
    "comments_zst_path = \"../Data/climbharder_comments.zst\"\n",
    "output_path_submissions = \"../Data/climbharder_posts.txt\"\n",
    "output_path_comments = \"../Data/climbharder_comments.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4623817-ef54-4472-b24b-f53174a9b646",
   "metadata": {},
   "source": [
    "### Processing zst-compressed ndj files\n",
    "For this step, I read the zst into a buffer a few lines at a time. For each line, I load the post (or comment) and filter it for sufficient karma threshold and text length. Then, I write it into the output file on its own line. My goal is to create one large newline-separated txt file of all post/comment content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb828ca-cc1c-4615-863e-05a68aefd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_pushshift_zst(input_path, output_path, subreddit_filter, score_threshold):\n",
    "    if not os.path.isfile(input_path):\n",
    "        print(f\"Error: File '{input_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    with open(input_path, 'rb') as f, open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(f)\n",
    "        buffer = b''\n",
    "        for chunk in iter(lambda: stream_reader.read(2**20), b''):\n",
    "            buffer += chunk\n",
    "            lines = buffer.split(b'\\n')\n",
    "            buffer = lines.pop()  # Save the last partial line for the next iteration\n",
    "\n",
    "            for line in lines:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                try:\n",
    "                    post = json.loads(line)\n",
    "                    if post.get('subreddit') != subreddit_filter:\n",
    "                        continue\n",
    "                    if post.get('score', 0) <= score_threshold:\n",
    "                        continue\n",
    "\n",
    "                    # For submissions\n",
    "                    if 'title' in post:\n",
    "                        title = post.get('title', '').strip()\n",
    "                        body = post.get('selftext', '').strip()\n",
    "                        full_text = f\"{title} {body}\".strip()\n",
    "                    # For comments\n",
    "                    else:\n",
    "                        full_text = post.get('body', '').strip()\n",
    "\n",
    "                    if len(full_text) < 120:\n",
    "                        continue\n",
    "\n",
    "                    out_file.write(full_text + '\\n')\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35366dd1-7b07-49fc-9e18-ad35b219b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pushshift_zst(\n",
    "        input_path=submissions_zst_path,\n",
    "        output_path=output_path_submissions,\n",
    "        subreddit_filter='climbharder',\n",
    "        score_threshold=20\n",
    "    )\n",
    "\n",
    "process_pushshift_zst(\n",
    "        input_path=comments_zst_path,\n",
    "        output_path=output_path_comments,\n",
    "        subreddit_filter='climbharder',\n",
    "        score_threshold=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b030889-52db-4230-99ce-2acc1b6abe5e",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "Since it's easier (and safer) to process these two massive zst files into separate txts using the above script, I need to combine them into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f7bfa9-a181-4c08-8906-15fafd06cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_two_txt_files(source_path1, source_path2, destination_path):\n",
    "    try:\n",
    "        with open(destination_path, 'w', encoding='utf-8') as dest_file:\n",
    "            for source_path in [source_path1, source_path2]:\n",
    "                with open(source_path, 'r', encoding='utf-8') as source_file:\n",
    "                    for line in source_file:\n",
    "                        dest_file.write(line)\n",
    "        print(f\"Successfully wrote combined contents to '{destination_path}'.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e2fcc1-5475-4b16-8dd8-aef2e8da71b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote combined contents to '../Data/climbharder_posts_and_comments.txt'.\n"
     ]
    }
   ],
   "source": [
    "output_posts_and_comments = \"../Data/climbharder_posts_and_comments.txt\"\n",
    "concatenate_two_txt_files(output_path_submissions, output_path_comments, output_posts_and_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ea64f-8611-46f7-b9e2-b572b977bad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
